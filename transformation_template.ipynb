{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from custom_utils.dp_storage.connector import mount, unmount_if_prod\n",
    "from custom_utils.dp_storage.writer import merge_and_upload\n",
    "from custom_utils import adf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = mount(dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Get the parameters from adf. When developing, insert values in the text widgets.\n",
    "source_config = adf.get_parameter_json('SourceConfig')\n",
    "destination_config = adf.get_parameter_json('DestinationConfig')\n",
    "source_folder_path = adf.get_parameter('SourceFolderPath')  # Remember that it has the format \"<container>/<directory>\"\n",
    "source_filename = adf.get_parameter('SourceFileName')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41271e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key columns\n",
    "timestamp_column = 'HIST_TIMESTAMP'\n",
    "index_columns = [timestamp_column, 'TODO: Add all key columns']\n",
    "time_resolution_egress = 'hour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that config and trigger parameters are aligned\n",
    "container_from_trigger = source_folder_path.split('/')[0]\n",
    "guid_from_trigger = source_folder_path.split('/')[1]\n",
    "directory_from_trigger = '/'.join(source_folder_path.split('/')[1:])  # Remember that folderPath from an ADF trigger has the format \"<container>/<directory>\"\n",
    "\n",
    "assert container_from_trigger == source_config['TODO: dataset_identifier']['container']\n",
    "assert guid_from_trigger == source_config['TODO: dataset_identifier']['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422d9b",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.path.join(mount_point, directory_from_trigger, source_filename)\n",
    "\n",
    "df = spark.read.parquet(source_path)\n",
    "# df = spark.read.option(\"delimiter\", \",\").csv(source_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5156013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa14e",
   "metadata": {},
   "source": [
    "Transform the data here. Follow this style guide: https://github.com/palantir/pyspark-style-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922f783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc6b0b6",
   "metadata": {},
   "source": [
    "## Merge and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_upload(\n",
    "    df,\n",
    "    destination_config['TODO: insert identifier name']['dataset'],\n",
    "    timestamp_column,\n",
    "    index_columns,\n",
    "    time_resolution_egress,\n",
    "    mount_point,\n",
    "    spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep this at the end of your notebook\n",
    "unmount_if_prod(mount_point, dbutils)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
